{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has for loop to go through hyperparameters\n",
    "\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from transformer import Models\n",
    "from transformer import Beam\n",
    "from transformer import Translator\n",
    "from transformer.Optim import ScheduledOptim\n",
    "\n",
    "from tools import CharacterTable\n",
    "from translator import SignalTranslator\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting the loss in the middle of the sequence\n",
    "# Different lengths for protein in training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants. Don't change these. \n",
    "pad = 0\n",
    "stop = 2\n",
    "start = 1\n",
    "max_in = 107\n",
    "max_out = 72\n",
    "n_chars = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = [(256, 32, 1000, 6, 6, 0.4, 64, 100, 1e-4, -0.03), (256, 64, 1000, 6, 6, 0.3, 64, 100, 1e-4, -0.03)]\n",
    "\n",
    "# change dim, change # heads\n",
    "\n",
    "for h in hyper:\n",
    "    # Hyperparameters. Do change these\n",
    "    d_model = h[0]\n",
    "    batch_size = h[1]\n",
    "    batches = h[2] # batches / epoch\n",
    "    n_warmup_steps = h[2] * 25\n",
    "    n_layers = h[3]\n",
    "    n_head = h[4]\n",
    "    dropout = h[5]\n",
    "    d_k = h[6]\n",
    "    epochs = h[7]\n",
    "    lr_max = h[8]\n",
    "    decay_power = h[9]\n",
    "\n",
    "    # Name for the model checkpoints\n",
    "    # Change this, probably to reflect the hyperparameters chosen above\n",
    "    hypers = [d_model, n_warmup_steps, batch_size, n_layers, n_head, dropout, d_k, epochs, lr_max, decay_power]\n",
    "    chkpt_name = '_'.join([str(h) for h in hypers])\n",
    "\n",
    "    model_opt = argparse.Namespace()\n",
    "    model_opt.src_vocab_size = n_chars\n",
    "    model_opt.tgt_vocab_size = n_chars\n",
    "    model_opt.max_token_seq_len = max_in\n",
    "    model_opt.proj_share_weight = True\n",
    "    model_opt.embs_share_weight = True\n",
    "    model_opt.d_k = d_k\n",
    "    model_opt.d_v = d_k\n",
    "    model_opt.d_model = d_model\n",
    "    model_opt.d_word_vec = d_model\n",
    "    model_opt.d_inner_hid = 2 * d_model\n",
    "    model_opt.n_layers = n_layers\n",
    "    model_opt.n_head = n_head\n",
    "    model_opt.dropout = dropout\n",
    "\n",
    "    ##############################\n",
    "    # Change this to use the GPU #\n",
    "    model_opt.cuda = True        #\n",
    "    ##############################\n",
    "\n",
    "    optim_opt = argparse.Namespace()\n",
    "    optim_opt.n_warmup_steps = n_warmup_steps\n",
    "    optim_opt.optim = optim.Adam\n",
    "    optim_opt.lr_max = lr_max\n",
    "    optim_opt.decay_power = decay_power\n",
    "    optim_opt.d_model = None\n",
    "\n",
    "    trans_opt = argparse.Namespace()\n",
    "    trans_opt.beam_size = 1\n",
    "    trans_opt.n_best = 1\n",
    "    trans_opt.max_trans_length = max_out\n",
    "\n",
    "    with open('../outputs/ctable_token.pkl', 'rb') as f:\n",
    "        ctable = pickle.load(f)\n",
    "\n",
    "    trans_opt.ctable = ctable\n",
    "\n",
    "    clf = SignalTranslator(model_opt, optim_opt, trans_opt)\n",
    "\n",
    "    steps = np.arange(epochs * batches)\n",
    "    lrs = clf.optimizer.get_learning_rate(steps)\n",
    "    _ = plt.plot(steps, lrs)\n",
    "\n",
    "    train_file = h5py.File('../data/train_tokens.hdf5')\n",
    "    val_file = h5py.File('../data/validate_tokens.hdf5')\n",
    "    history = clf.train(train_file, val_file, epochs=epochs, batch_size=batch_size,\n",
    "                               save_model='../outputs/models/' + chkpt_name, save_mode='best')\n",
    "    train_file.close()\n",
    "    val_file.close() #save model and load model\n",
    "\n",
    "    # See how well it does on first batch_size test sequences\n",
    "    file = h5py.File('../data/test_tokens.hdf5')\n",
    "    training_data = SignalTranslator.generator_from_h5(file, batch_size, shuffle=False, use_cuda=True)\n",
    "    src, tgt = next(training_data)\n",
    "    file.close()\n",
    "    %time decoded, all_hyp, all_scores = clf.translate_batch(src)\n",
    "    for tg, dec in zip(tgt[0], decoded):\n",
    "        print(dec)\n",
    "        print(ctable.decode(tg.data.cpu().numpy())[:])\n",
    "        print()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    x = np.arange(len(history['train_loss']))\n",
    "    _ = axs[0].plot(x, history['train_loss'], label='training loss', alpha=0.8)\n",
    "    _ = axs[0].plot(x, history['val_loss'], label='validation loss', alpha=0.8)\n",
    "    _ = axs[0].legend()\n",
    "    _ = axs[1].plot(x, history['train_acc'], label='training accuracy', alpha=0.8)\n",
    "    _ = axs[1].plot(x, history['val_acc'], label='validation accuracy', alpha=0.8)\n",
    "    _ = axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For loading the model\n",
    "#chkpt = \"../outputs/models/\" + chkpt_name + \".chkpt\"\n",
    "#clf = SignalTranslator.load_model(chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()\n",
    "# see how similar the SPs that have been predicted well are to the ones in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare similarity of SPs predicted well by model to those in training set USING STRINGS\n",
    "# See how well it does on first batch_size test sequences \n",
    "file = h5py.File('../data/test_tokens.hdf5')\n",
    "test_data = SignalTranslator.generator_from_h5(file, batch_size, shuffle=False, use_cuda=True)\n",
    "src, tgt = next(test_data) #src prot, tgt = signal peptides\n",
    "file.close()\n",
    "%time decoded, all_hyp, all_scores = clf.translate_batch(src)\n",
    "\n",
    "good_pred = [] # stores signal peptides predicted well that are in training set\n",
    "\n",
    "for tg, tg2, dec in zip(tgt[0], src[0], decoded):\n",
    "    actual = ctable.decode(tg.data.cpu().numpy())[:]\n",
    "    print()\n",
    "    print(dec)\n",
    "    print(actual)\n",
    "    print()\n",
    "    \n",
    "    # sum returns length of amino acids that are the same at same position? --> so full length returned\n",
    "    # means its the same SP\n",
    "    # plot signal peptide to similarity\n",
    "    # plot top three simmilar to how similar their SPs areq\n",
    "    \n",
    "    actual = ''.join(actual.split())\n",
    "    actual = actual[1:-1]\n",
    "    dec = dec[:-1]\n",
    "    \n",
    "    if abs(len(dec) - len(actual)) <= 5 and similar(actual, dec) >= 0.7:\n",
    "        good_pred.append((tg, tg2, dec, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(good_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare prots of SPs predicted well to prots in training dataset\n",
    "file = h5py.File('../data/train_tokens.hdf5')\n",
    "\n",
    "# checking for similar protein sequences\n",
    "index = 1\n",
    "\n",
    "for tg, tg2, dec, actual in good_pred:\n",
    "    train_data = SignalTranslator.generator_from_h5(file, batch_size, shuffle=False, use_cuda=True)\n",
    "    sum_length = 0 # length of same amino acids when comparing SPs/prots predicted well to training set\n",
    "    sim = [] # stores signal peptides predicted well that are in training set\n",
    "    lengths = [] # stores all protein sequences of the signal peptides predicted well that are in training set\n",
    "    most_sim = []\n",
    "    lst = []\n",
    "    int1 = 0\n",
    "    int2 = 0\n",
    "    int3 = 0\n",
    "\n",
    "    print(\"PROTEIN {}\".format(index))\n",
    "    print(dec)\n",
    "    print(actual)\n",
    "    test_prot = tg2.data.cpu().numpy()\n",
    "    \n",
    "    for i, batch in enumerate(train_data): # each batch in training data\n",
    "        src, tgt = batch\n",
    "        train_prot = src[0].data.cpu().numpy() # training proteins\n",
    "        for row in train_prot:\n",
    "            sum_length = sum(test_prot == row) # length of amino acids that are the same\n",
    "            #print(sum_length)\n",
    "            if sum_length > 51:\n",
    "                sim.append((row, sum_length / 102))\n",
    "    \n",
    "    if len(sim) > 0:\n",
    "        for key in sim:\n",
    "            prot, length = key\n",
    "            lengths.append(length)\n",
    "        \n",
    "        # clone the list by slicing, find top 3 lengths\n",
    "        copy = lengths[:]\n",
    "        \n",
    "        int1 = max(copy)\n",
    "        copy = [x for x in copy if x != int1]\n",
    "            \n",
    "        if len(copy) > 0:\n",
    "            int2 = max(copy)\n",
    "            copy = [x for x in copy if x != int2]\n",
    "            \n",
    "        if len(copy) > 0:\n",
    "            int3 = max(copy)\n",
    "        \n",
    "        lst = [int1, int2, int3]\n",
    "        for pair in sim:\n",
    "            prot, length = pair\n",
    "            if length in lst:\n",
    "                print((prot, length))\n",
    "                lst.remove(length)\n",
    "            if len(lst) == 0:\n",
    "                break\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    index += 1\n",
    "    \n",
    "# sum returns length of amino acids that are the same at same position? --> so full length returned\n",
    "# means its the same SP\n",
    "# plot signal peptide to similarity\n",
    "# plot top three simmilar to how similar their SPs areq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare SPs predicted well to SPs in training dataset\n",
    "file = h5py.File('../data/train_tokens.hdf5')\n",
    "\n",
    "# checking for similar protein sequences\n",
    "index = 1\n",
    "\n",
    "for tg, tg2, dec, actual in good_pred:\n",
    "    train_data = SignalTranslator.generator_from_h5(file, batch_size, shuffle=False, use_cuda=True)\n",
    "    sum_length = 0 # length of same amino acids when comparing SPs/prots predicted well to training set\n",
    "    sim = [] # stores signal peptides predicted well that are in training set\n",
    "    lengths = [] # stores all protein sequences of the signal peptides predicted well that are in training set\n",
    "    most_sim = []\n",
    "    int1 = 0\n",
    "    int2 = 0\n",
    "    int3 = 0\n",
    "    lst = []\n",
    "\n",
    "    print(\"PROTEIN {}\".format(index))\n",
    "    print(dec)\n",
    "    print(actual)\n",
    "    test_sp = tg.data.cpu().numpy()\n",
    "    \n",
    "    for i, batch in enumerate(train_data): # each batch in training data\n",
    "        src, tgt = batch\n",
    "        train_sp = tgt[0].data.cpu().numpy() # training proteins\n",
    "        for row in train_sp:\n",
    "            sum_length = sum(test_sp == row) # length of amino acids that are the same\n",
    "            if sum_length > 51:\n",
    "                sim.append((row, sum_length / 102))\n",
    "    \n",
    "    if len(sim) > 0:\n",
    "        for key in sim:\n",
    "            sp, length = key\n",
    "            lengths.append(length)\n",
    "        \n",
    "        # clone the list by slicing, find top 3 lengths\n",
    "        copy = lengths[:]\n",
    "        \n",
    "        int1 = max(copy)\n",
    "        copy = [x for x in copy if x != int1]\n",
    "            \n",
    "        if len(copy) > 0:\n",
    "            int2 = max(copy)\n",
    "            copy = [x for x in copy if x != int2]\n",
    "            \n",
    "        if len(copy) > 0:\n",
    "            int3 = max(copy)\n",
    "        \n",
    "        lst = [int1, int2, int3]\n",
    "        for pair in sim:\n",
    "            prot, length = pair\n",
    "            if length in lst:\n",
    "                print((prot, length))\n",
    "                lst.remove(length)\n",
    "            if len(lst) == 0:\n",
    "                break\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    index += 1\n",
    "    \n",
    "# sum returns length of amino acids that are the same at same position? --> so full length returned\n",
    "# means its the same SP\n",
    "# plot signal peptide to similarity\n",
    "# plot top three simmilar to how similar their SPs areq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare similarity of SPs predicted well by model to those in training set USING STRINGS\n",
    "# See how well it does on first batch_size test sequences \n",
    "file = h5py.File('../data/test_tokens.hdf5')\n",
    "test_data = SignalTranslator.generator_from_h5(file, batch_size, shuffle=False, use_cuda=True)\n",
    "src, tgt = next(test_data) #src prot, tgt = signal peptides\n",
    "file.close()\n",
    "%time decoded, all_hyp, all_scores = clf.translate_batch(src)\n",
    "\n",
    "acc = [] # stores all signal peptides predicted well\n",
    "sim = [] # stores signal peptides predicted well that are in training set\n",
    "same_prot = [] # stores all protein sequences of the signal peptides predicted well that are in the training set\n",
    "\n",
    "for tg, tg2, dec in zip(tgt[0], src[0], decoded):\n",
    "    actual = ctable.decode(tg.data.cpu().numpy())[:]\n",
    "    #prot_seq = ctable.decode(tg2.data.cpu().numpy())[:]\n",
    "    print()\n",
    "    #print(prot_seq)\n",
    "    print(dec)\n",
    "    print(actual)\n",
    "    print()\n",
    "    \n",
    "    # sum returns length of amino acids that are the same at same position? --> so full length returned\n",
    "    # means its the same SP\n",
    "    # plot signal peptide to similarity\n",
    "    # plot top three simmilar to how similar their SPs areq\n",
    "    \n",
    "    actual = ''.join(actual.split())\n",
    "    actual = actual[1:-1]\n",
    "    dec = dec[:-1]\n",
    "    \n",
    "    if abs(len(dec) - len(actual)) <= 5 and similar(actual, dec) >= 0.7:\n",
    "        acc.append(actual)\n",
    "        if actual in train_sp:\n",
    "            sim.append(actual)\n",
    "    \n",
    "    #if prot_seq in train_prot:\n",
    "    #    same_prot.append(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
